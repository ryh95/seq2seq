\begin{abstract}

深度神经网络（DNN）是一个能在困难学习任务上表现卓越的模型。尽管在庞大的带标签训练集可获得的时候深度神经网络能够表现的很好，但是深度神经网络依然不能把一个序列映射到另一个序列。在本文中，我们提出了一个端到端的的序列学习方法，能够在序列结构上做最小的假设。我们的方法是使用多层LSTM将输入序列映射到一个固定维度的向量，然后使用另一个深层LSTM对向量解码，得到目标序列。我们的主要结果是，在WMT-14数据集上的英语到法语的翻译任务上，LSTM产生的翻译在整个测试集上获得了34.8的BLEU得分，LSTM的BLEU分数受到词汇外词语的惩罚。此外LSTM在长句子的翻译上没有遇到困难，作为比较基于短语的SMT系统在相同的数据集上得到了33.3的BLEU得分。当我们使用LSTM重新排列由上述SMT系统产生的1000个假设时，其BLEU得分增加到36.5，这接近以前的最好水平。LSTM还学习了敏感的词语和句子表示，它们对词序是敏感的，并且对于主动和被动语态是保持不变的。最后，我们发现颠倒所有输入语句（但不是目标句子）的单词顺序能够提高LSTM的性能，因为这样做在输入句子和目标句子之间引入了许多短期依赖，使得优化问题更容易。
\end{abstract}
