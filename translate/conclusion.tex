\section{结论}

在这项工作中，我们展示了一个有限词汇量的大的深层LSTM表现胜过一个标准的基于SMT的系统，在大规模的MT任务中词汇可以是无限的。我们在机器翻译上的简单的基于LSTM的方法的成功表明，只要他们有足够的训练数据，它应该在许多其他序列学习问题上做得很好。

我们对通过翻转源语句中的词的顺序而获得的改进的程度感到惊讶。我们得出结论，找到一个具有最大数量的短期依赖问题编码是重要的，因为它们使学习问题更简单。特别地，尽管我们不能在非反向翻译问题（图\ref{fig:translation-model2}所示）上训练标准RNN，我们认为，当源语句颠倒时，标准的RNN应该很容易训练
（虽然我们没有通过实验验证）。

我们也对LSTM正确翻译很长句子的能力感到惊讶。我们最初相信LSTM将由于其有限的记忆会在长句子上失败，并且其他研究人员报告过与我们类似的模型在长句子上差的性能\cite{cho14,bog14,curse}。然而，对反转数据集上训练的LSTM在翻译长句子上没有什么困难。

最重要的是，我们展示了一个简单，直接和相对未优化的方法，这个方法可以超越成熟的SMT系统，因此进一步的工作可能会进一步提高翻译精度。这些结果表明，我们的方法可能在其他具有挑战的序列问题上表现很好。
